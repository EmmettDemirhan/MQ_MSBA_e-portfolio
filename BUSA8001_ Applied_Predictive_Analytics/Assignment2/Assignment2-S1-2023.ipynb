{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Umut Demirhan \n",
    "\n",
    "Student ID: 46739106"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business Analytics - Assignment 2  \n",
    "\n",
    "**Assignment Points**: 100  \n",
    "**Submission**: Provide your answers in this notebook and submit it via iLearn\n",
    "\n",
    "- Where a question requires a written (text) solution provide your answer in Markdown in appropriate cells under each question.\n",
    "- Comment out your print statements unless you are explicitly asked to use the print() function. \n",
    "- 5 marks will be deducted for printed outputs that are not asked for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About the Assignment\n",
    "\n",
    "- Assignment 2 extends Assignment 1 on credit card applications. \n",
    "\n",
    "\n",
    "- For this assignment there are two files in the `data` folder `credit_record.csv` and `application_record.csv` where bank clients are related by the `ID` column.\n",
    "- In `credit_record.csv` we have the following variables\n",
    "\n",
    "| Feature Name         | Explanation     | Additional Remarks |\n",
    "|--------------|-----------|-----------|\n",
    "| ID | Randomly allocated client number      |         |\n",
    "| AMT_INCOME_TOTAL   | Annual income  |  |\n",
    "| NAME_INCOME_TYPE   | Income Source |  |\n",
    "| NAME_EDUCATION_TYPE   | Level of Education  |  |\n",
    "| CODE_GENDER   | Applicant's Gender   |  |\n",
    "| FLAG_OWN_CAR | Car Ownership |  | \n",
    "| CNT_CHILDREN | Number of Children | |\n",
    "| FLAG_OWN_REALTY | Real Estate Ownership | | \n",
    "| NAME_FAMILY_STATUS | Relationship Status | | \n",
    "| NAME_HOUSING_TYPE | Housing Type | | \n",
    "| DAYS_BIRTH | No. of Days | Count backwards from current day (0), -1 means yesterday\n",
    "| DAYS_EMPLOYED | No. of Days | Count backwards from current day(0). If positive, it means the person is currently unemployed.\n",
    "| FLAG_MOBIL | Mobile Phone Ownership | | \n",
    "| FLAG_WORK_PHONE | Work Phone Ownership | | \n",
    "| FLAG_PHONE | Landline Phone Ownership | | \n",
    "| FLAG_EMAIL | Landline Phone Ownership | | \n",
    "| OCCUPATION_TYPE | Occupation | | \n",
    "| CNT_FAM_MEMBERS | Count of Family Members | |\n",
    "\n",
    "\n",
    "\n",
    "- In `credit_record.csv` we have the following variables\n",
    "\n",
    "\n",
    "| Feature Name         | Explanation     | Additional Remarks |\n",
    "|--------------|-----------|-----------|\n",
    "| ID | Randomly allocated client number | |\n",
    "| MONTHS_BALANCE | Number of months in the past from now when STATUS is measured | 0 = current month, -1 = last month, -2 = two months ago, etc.|\n",
    "| STATUS | Number of days a payment is past due | 0: 1-29 days past due 1: 30-59 days past due 2: 60-89 days overdue 3: 90-119 days overdue 4: 120-149 days overdue 5: Overdue or bad debts, write-offs for more than 150 days C: paid off that month X: No loan for the month |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "### Task 1: Reading, Summarising and Cleaning Data (Total Marks: 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Question 1.** \n",
    "\n",
    "1. Import the `application_record.csv` and `credit_record.csv` files from `data` folder into pandas DataFrames named `df_application` and `df_credit`, respectively. (1 mark)\n",
    "\n",
    "2. How many rows are there in `df_application` and `df_credit`, respectively? Answer using both print() function and in Markdown text. (1 mark)\n",
    "\n",
    "3. How many unique bank clients are there in `df_application` and `df_credit`? Answer using both print() function and in Markdown text. (1 mark)\n",
    "\n",
    "4. Add the records from `df_credit` to `df_application` by merging the data from the two DataFrames on the `ID` column, and output the joint data into a new DataFrame named `df`. Hint: Use `merge` function from pandas by setting `how` parameter to `inner` (4 marks) \n",
    "\n",
    "5. How many rows and how many unique clients are there in `df`? (1 mark)\n",
    "\n",
    "6. How are multiple rows for each `ID` in `df` different? Answer in Markdown text. (2 mark) \n",
    "\n",
    "(10 marks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in df_application:  438557\n",
      "Number of rows in df_credit:  1048575\n",
      "Number of unique bank clients in df_application:  438510\n",
      "Number of unique bank clients in df_credit:  45985\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Answer 1.1\n",
    "df_application = pd.read_csv('data/application_record.csv')\n",
    "df_credit = pd.read_csv('data/credit_record.csv')\n",
    "\n",
    "# Amswer 1.2\n",
    "print(\"Number of rows in df_application: \",   len(df_application))\n",
    "print(\"Number of rows in df_credit: \", len(df_credit))\n",
    "\n",
    "# Answer 1.3\n",
    "print(\"Number of unique bank clients in df_application: \", df_application['ID'].nunique())\n",
    "print(\"Number of unique bank clients in df_credit: \", df_credit['ID'].nunique())\n",
    "\n",
    "# Answer 1.4\n",
    "df = pd.merge(df_application, df_credit, on='ID', how='inner')\n",
    "\n",
    "# Answer 1.5 \n",
    "#Number of rows in df:\n",
    "#df.shape[0]\n",
    "#Number of unique clients in df:\n",
    "#df['ID'].nunique()\n",
    "#df['MONTHS_BALANCE'].unique()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Answer 1.2 \n",
    "* Number of rows in 'df_application' is 438557 and, number of rows in 'df_credit' is 1048575\n",
    "###### Answer 1.3\n",
    "* Number of unique bank clients in  'df_application' is 438510 and number of unique bank clients in 'df_credit' is 45985\n",
    "###### Answer 1.5\n",
    "* Number of rows in 'df' is 777715 and, number of unique clients in 'df' is 36457\n",
    "\n",
    "###### Answer 1.6\n",
    "* Because we merged the two data frames, the multiple rows for each ID in df indicate that there are multiple records for each client. Therefore, if we delete the 'month balance' and 'status' columns, then 'df' will have many duplicate rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Question 2.**\n",
    "\n",
    "1. Change the values of `STATUS` in `df` according to the following mapping: {C, X, 0} -> 0 and {1, 2, 3, 4, 5} -> 1 making sure that the new values of 0 and 1 are encoded as integers. (2 marks)\n",
    "2. Create a new numpy array named `list_of_defaults` containing *unique* `ID` numbers for the clients who have `STATUS` = 1 in any of the last 12 months in the dataset. (2 marks) \n",
    "3. Create a new DataFrame called `df_final` that contains the rows of `df` for which the `ID` are in `list_of_defaults`, keeping only one row for each `ID` (i.e. eliminate rows with duplicate `ID`s while keeping the first duplicate row). How many rows do you have in `df_final`? Answer using both print() function and in Markdown text. (Hint: find out about `isin()` function in pandas.) (2 marks)\n",
    "4. Add a new column `y = 1` for all the rows in `df_final`. (1 marks)\n",
    "5. Increase `df_final` to a total of 4,000 rows by adding rows from `df` with unique `ID`s (nonduplicated `ID`s) which are not in `list_of_defaults`. To do this start adding the rows from the beginning of `df`. (Hint: learn what `~`, i.e. tilde sign, does in pandas). (2 marks) \n",
    "6. Fill the missing values of `y` in `df_final` with zeros. Remove `STATUS` and `MONTHS_BALANCE` from `df_final`. How many clients with  overdue payments of more than 29 days and how many clients with less than 29 days overdue payments are there in `df_final`? Answer using both print() function and in Markdown text.(1 mark)\n",
    "\n",
    "(10 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in df_final: 1721\n",
      "Number of clients with overdue payments of more than 29 days: 1721\n",
      "Number of clients with overdue payments of less than or equal to 29 days: 2279\n"
     ]
    }
   ],
   "source": [
    "# Answer 2.1\n",
    "df['STATUS'] = df['STATUS'].replace(['C', 'X', 0], 0).replace([1, 2, 3, 4, 5], 1).astype(int)\n",
    "# Answer 2.2\n",
    "list_of_defaults = df[df['MONTHS_BALANCE'] >= -12]['ID'][df['STATUS'] == 1].unique()\n",
    "# Answer 2.3\n",
    "df_final = df[df['ID'].isin(list_of_defaults)].drop_duplicates(subset='ID', keep='first')\n",
    "print(\"Number of rows in df_final:\", len(df_final))\n",
    "# Answer 2.4\n",
    "df_final['y'] = 1\n",
    "# Answer 2.5\n",
    "df_nondefault = df[~df['ID'].isin(list_of_defaults)].drop_duplicates(subset='ID', keep='first')    \n",
    "df_final = pd.concat([df_final, df_nondefault.iloc[:4000-df_final.shape[0]]])\n",
    "# Answer 2.6\n",
    "df_final['y'] = df_final['y'].fillna(0)\n",
    "df_final = df_final.drop(['STATUS', 'MONTHS_BALANCE'], axis=1)\n",
    "overdue_29 = df_final[df_final['y'] > 0]['ID'].nunique()\n",
    "under_29 = df_final[df_final['y'] == 0]['ID'].nunique()\n",
    "print(\"Number of clients with overdue payments of more than 29 days:\", overdue_29)\n",
    "print(\"Number of clients with overdue payments of less than or equal to 29 days:\", under_29)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Answer 2.3\n",
    "* Number of rows in 'df_final' is 1721\n",
    "###### Answer 2.6\n",
    "* Number of clients with overdue payments of more than 29 days is 1721\n",
    "* Number of clients with overdue payments of less than or equal to 29 days is 2279"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:25%;margin-left:0;\"> \n",
    "\n",
    "**Question 3**. \n",
    "- Delete `ID` column from `df_final` (1 marks)\n",
    "- Of the remaining variables in `df_final` and assuming that `NAME_EDUCATION_TYPE` is the only ordinal variable, how many variable are of numeric and nominal types? Provide lists of all numeric and nominal variables. (6)\n",
    "- Using an appropriate function find and comment on the missing values in `df_final`, i.e. how many variables and how many observations? (3 marks)   \n",
    "(10 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Answer 3.1\n",
    "df_final.drop('ID', axis=1, inplace=True)\n",
    "# Answer 3.2\n",
    "num_var = list(df_final.select_dtypes(include=['int64', 'float64']).columns)\n",
    "nom_var = list(df_final.select_dtypes(include=['object', 'category']).columns)\n",
    "nom_var.remove('NAME_EDUCATION_TYPE')\n",
    "#len(num_var)\n",
    "#len (nom_var)\n",
    "# Answer 3.3\n",
    "# df_final.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Answer 3.2\n",
    "\n",
    "* There are 10 numeric varible including newly created variable 'y'\n",
    "    * List of  all numeric variables;\n",
    "['CNT_CHILDREN',|\n",
    " 'AMT_INCOME_TOTAL',\n",
    " 'DAYS_BIRTH',\n",
    " 'DAYS_EMPLOYED',\n",
    " 'FLAG_MOBIL',\n",
    " 'FLAG_WORK_PHONE',\n",
    " 'FLAG_PHONE',\n",
    " 'FLAG_EMAIL',\n",
    " 'CNT_FAM_MEMBERS',\n",
    " 'y']\n",
    "\n",
    "* There are 7 nominal varible exluding 'NAME_EDUCATION_TYPE' as it is ordinal variable\n",
    "    * List of  all nominal variables;\n",
    "['CODE_GENDER',\n",
    " 'FLAG_OWN_CAR',\n",
    " 'FLAG_OWN_REALTY',\n",
    " 'NAME_INCOME_TYPE',\n",
    " 'NAME_FAMILY_STATUS',\n",
    " 'NAME_HOUSING_TYPE',\n",
    " 'OCCUPATION_TYPE']\n",
    "\n",
    "###### Answer 3.3\n",
    "\n",
    "* Three variables have missing values which are 'CNT_CHILDREN', 'NAME_EDUCATION_TYPE', and 'OCCUPATION_TYPE' \n",
    "* They have 74, 1831 and 1152 missing observations respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "### Task 2: Imputing missing values and dealing with categorical features (Total Marks: 30)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4.** \n",
    "- Use an appropriate `pandas` function to impute missing values in `df_final` (10 marks)\n",
    "    - Be careful when deciding which method to use to replace missing observations \n",
    "    - Take into consideration the type of each variable and the best practices we discussed in class/lecture notes\n",
    "- Briefly explain what you have done and why. (5 marks)\n",
    "\n",
    "(Total: 15 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Replace missing values in CNT_CHILDREN column with 0\n",
    "df_final['CNT_CHILDREN'].fillna(0, inplace=True)\n",
    "\n",
    "# Replace missing values in NAME_EDUCATION_TYPE and OCCUPATION_TYPE columns with the mode\n",
    "df_final['NAME_EDUCATION_TYPE'].fillna(df_final['NAME_EDUCATION_TYPE'].mode()[0], inplace=True)\n",
    "df_final['OCCUPATION_TYPE'].fillna(df_final['OCCUPATION_TYPE'].mode()[0], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Answer 4\n",
    "* For the 'CNT_CHILDREN' column, I replaced the missing values with 0, assuming that clients with missing values have no children. \n",
    "* For the 'NAME_EDUCATION_TYPE' and 'OCCUPATION_TYPE' columns, I used the mode() method to compute the mode of the non-missing values, and replace the missing values with the mode. Because,  This is a common approach for imputing missing values in categorical variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:25%;margin-left:0;\"> \n",
    "\n",
    "**Question 5**. Convert the values in `NAME_EDUCATION_TYPE` as follows\n",
    "- Lower secondary -> 1\n",
    "- Secondary / secondary special -> 2\n",
    "- Incomplete higher -> 3\n",
    "- Higher education -> 4\n",
    "\n",
    "\n",
    "(Total: 5 marks)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 5\n",
    "education_dict = {\n",
    "    'Lower secondary': 1,\n",
    "    'Secondary / secondary special': 2,\n",
    "    'Incomplete higher': 3,\n",
    "    'Higher education': 4\n",
    "}\n",
    "\n",
    "df_final['NAME_EDUCATION_TYPE'] = df_final['NAME_EDUCATION_TYPE'].map(education_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:25%;margin-left:0;\"> \n",
    "\n",
    "**Question 6**. \n",
    "\n",
    "Add dummy variables to `df_final` for all of the nominal features which are currently stored as string (text). \n",
    "- Make sure to delete the original variables from the dataframe\n",
    "- Drop the first column from each set of created dummy variable, i.e. for each feature\n",
    "\n",
    "\n",
    "\n",
    "(Total: 10 marks)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in nom_var:\n",
    "    dummies = pd.get_dummies(df_final[feature], prefix=feature, drop_first=True)\n",
    "    df_final = pd.concat([df_final, dummies], axis=1)\n",
    "    df_final.drop(columns=[feature], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "### Task 3 Preparing X and y arrays (Total Marks: 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 7**. \n",
    "\n",
    "- Create a numpy array named `y` from the `y` column of `df_final` making sure that the values of the array `y` are stored as integers (3 marks)   \n",
    "- Create a numpy array named `X`  from all the remaining features in `df_final` (2 marks)   \n",
    "\n",
    "(Total: 5 Marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 7\n",
    "y = np.array(df_final['y'], dtype=int)\n",
    "\n",
    "X = np.array(df_final.drop('y', axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:25%;margin-left:0;\"> \n",
    "\n",
    "**Question 8**. \n",
    "\n",
    "- Use an appropriate scikit-learn library we used in class to create `y_train`, `y_test`, `X_train` and `X_test` by splitting the data into 70% train and 30% test datasets (2.5 marks) \n",
    "    - Set random_state to 7 and stratify the subsamples so that train and test datasets have roughly equal proportions of the target's class labels \n",
    "- Standardise the data using `StandardScaler` library (2.5 marks)   \n",
    "\n",
    "(Total: 5 marks) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 8\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Splitting the data into 70% train and 30% test datasets, stratified by the target variable\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=7, stratify=y)\n",
    "\n",
    "# Standardizing the data using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_std = scaler.fit_transform(X_train)\n",
    "X_test_std = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "### Task 4. Support Vector Classifier and Accuracies (Total Marks: 30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 9**. \n",
    "\n",
    "- Train a Support Vector Classifier on standardised data (3 marks)\n",
    "    - Use `linear` kernel and set `random_state` to 7 (don't change any other parameters)\n",
    "    - Compute and print training and test dataset accuracies\n",
    "- Train another Support Vector Classifier on standardised data (3 marks)\n",
    "    - Use `rbf` kernel and set `random_state` to 7 (don't change any other parameters)\n",
    "    - Compute and print training and test dataset accuracies\n",
    "- What can you say about the presence of nonlinearities in the dataset? (4 marks)\n",
    "\n",
    "(Total: 10 marks)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy with linear kernel: 65.32%\n",
      "Test accuracy with linear kernel: 64.58%\n",
      "Training accuracy with rbf kernel: 78.43%\n",
      "Test accuracy with rbf kernel: 71.42%\n"
     ]
    }
   ],
   "source": [
    "# Answer 9\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Training a Support Vector Classifier with linear kernel\n",
    "svm_linear = SVC(kernel='linear', random_state=7)\n",
    "svm_linear.fit(X_train_std, y_train)\n",
    "\n",
    "# Computing training and test dataset accuracies\n",
    "train_acc_linear = svm_linear.score(X_train_std, y_train)\n",
    "test_acc_linear = svm_linear.score(X_test_std, y_test)\n",
    "print(\"Training accuracy with linear kernel: {:.2f}%\".format(train_acc_linear*100))\n",
    "print(\"Test accuracy with linear kernel: {:.2f}%\".format(test_acc_linear*100))\n",
    "\n",
    "# Training a Support Vector Classifier with rbf kernel\n",
    "svm_rbf = SVC(kernel='rbf', random_state=7)\n",
    "svm_rbf.fit(X_train_std, y_train)\n",
    "\n",
    "# Computing training and test dataset accuracies\n",
    "train_acc_rbf = svm_rbf.score(X_train_std, y_train)\n",
    "test_acc_rbf = svm_rbf.score(X_test_std, y_test)\n",
    "print(\"Training accuracy with rbf kernel: {:.2f}%\".format(train_acc_rbf*100))\n",
    "print(\"Test accuracy with rbf kernel: {:.2f}%\".format(test_acc_rbf*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer 9\n",
    "Based on the provided accuracy scores above, we can conclude that the dataset likely contains non-linearities that cannot be captured by a linear model. The reason is that the accuracy scores obtained by the linear kernel are significantly lower than the scores obtained by the rbf kernel, which is a non-linear kernel. The higher accuracy achieved by the rbf kernel on both the training and test data suggests that it is better at capturing the underlying relationships in the data. Therefore, we can infer that the dataset likely contains non-linear relationships between the features and the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:25%;margin-left:0;\"> \n",
    "\n",
    "**Question 10**\n",
    "\n",
    "- Extract 2 linear principal components from the standardised features using an appropriate `sklearn` library (5 marks)\n",
    "- Train a Support Vector Classifier on the computed principal components (5 marks) \n",
    "    - Use `rbf` kernel and set `random_state` to 7 (don't change any other parameters)\n",
    "- Compute and print training and test dataset accuracies (5 marks)\n",
    "- What can you say about the ability of the 2 principal components to compress the information contained in the features matrix `X`, and why? (5 marks)     \n",
    "\n",
    "\n",
    "(Total: 20 marks)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA with 2 components:\n",
      "Training accuracy: 0.5721\n",
      "Test accuracy: 0.5717\n"
     ]
    }
   ],
   "source": [
    "# Answer 10\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "# Train a Support Vector Classifier on the computed principal components using rbf kernel\n",
    "svc_pca = SVC(kernel='rbf', random_state=7)\n",
    "svc_pca.fit(X_train_pca, y_train)\n",
    "\n",
    "train_acc_pca = svc_pca.score(X_train_pca, y_train)\n",
    "test_acc_pca = svc_pca.score(X_test_pca, y_test)\n",
    "\n",
    "print('PCA with 2 components:')\n",
    "print(f'Training accuracy: {train_acc_pca:.4f}')\n",
    "print(f'Test accuracy: {test_acc_pca:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer 10\n",
    "Based on the results obtained, it appears that the 2 principal components resulting from the PCA transformation were not able to fully capture the information contained in the features matrix X. The low training and test accuracies obtained with the SVC model trained on these principal components suggest that some important information may have been lost during the dimensionality reduction process.\n",
    "Therefore, it's possible that more principal components would be needed to fully capture the information contained in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
