{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business Analytics - Assignment 1 {-}\n",
    "\n",
    "Umut Demirhan\n",
    "46739106\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assignment Points**: 100  \n",
    "**Due Date**: Friday Week 4 (17 March) @ 11.59pm  \n",
    "**Submission**: Submit your file using the submission link on iLearn\n",
    "\n",
    "\n",
    "- Put **all your work** into this file;\n",
    "- Failure to supply solutions in the cells provided below each question will result in a loss of 30 points;\n",
    "- Follow all instructions closely and **do not** print your variables to the screen unless explicitly asked to do so;\n",
    "    - Comment out print statements where necessary and make sure that your submitted notebook does not have the output of previously executed print statements;\n",
    "    - 10 marks will be deducted for every redundant print statement not explicitly asked for.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About the Assignment\n",
    "\n",
    "Credit score cards are used as a risk control method in the financial industry. Personal information submitted by credit card applicants are used to predict the probability of future defaults. The bank employs such data to decide whether to issue a credit card to the applicant or not.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "| Feature Name         | Explanation     | Additional Remarks |\n",
    "|--------------|-----------|-----------|\n",
    "| ID | Randomly allocated client number      |         |\n",
    "| Income   | Annual income  |  |\n",
    "| Gender   | Applicant's Gender   | Male = 0, Female = 1  |\n",
    "| Car | Car Ownership | Yes = 1, No = 0 | \n",
    "| Children | Number of Children | |\n",
    "| Real Estate | Real Estate Ownership | Yes = 1, No = 0 \n",
    "| Days Since Birth | No. of Days | Count backwards from current day (0), -1 means yesterday\n",
    "| Days Employed | No. of Days | Count backwards from current day(0). If positive, it means the person is currently unemployed.\n",
    "| Payment Default | Whether a client has overdue credit card payments | Yes = 1, No = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Problem 1 - (50 points) {-}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1** \n",
    "\n",
    "- Import the `assignment_data.xlsx` file from `data` folder into a pandas DataFrame named `df`; \n",
    "- Delete duplicate rows from `df` according to `ID`;\n",
    "- Delete the `ID` column.\n",
    "- How many rows are left in `df`?\n",
    "\n",
    "(10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5976, 8)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the data\n",
    "import pandas as pd\n",
    "df = pd.read_excel('data/assignment_data.xlsx')\n",
    "\n",
    "# Deleting duplicate rows from df according to ID\n",
    "df.drop_duplicates(subset=\"ID\", inplace=True)\n",
    "\n",
    "#Deleting the ID column\n",
    "df.drop(columns=['ID'], inplace=True)\n",
    "\n",
    "#Number of rows lleft in df\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5976 rows are left after dublicates are removed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Question 2**\n",
    "\n",
    "- Reset the index in `df` using an appropriate function from `pandas` so that the new index corresponds to the number of rows (make sure to delete the old index). \n",
    "- How many positive values of `Days Employed` are there?\n",
    "- Replace the positive values of `Days Employed` with 0 (zero) in `df`\n",
    "\n",
    "(10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reseting the index in df\n",
    "df.reset_index(inplace= True,drop=True)\n",
    "\n",
    "#Number of positive values of Days Employed\n",
    "(df['Days Employed'] > 0).sum()\n",
    "\n",
    "#Replacing the positive values of Days Employed with 0  in df\n",
    "df.loc[df['Days Employed']>0,'Days Employed']=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* There were 967 positive values of Days Employed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Question 3**\n",
    "\n",
    "Create two new variables in `df` named \n",
    "\n",
    "1. `Age`;\n",
    "2. `Years in Employment`,\n",
    "\n",
    "which measure age and employment length in **years** (decimal numbers) from `Days Since Birth` and `Days Employed` by applying approapriate transformations on these variables. \n",
    "\n",
    "Delete the original variables `Days Since Birth` and `Days Employed`.\n",
    "\n",
    "(5 points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Creating new variable 'Age' and deleting column 'Days Since Birth'\n",
    "df['Age'] = -df['Days Since Birth']/365\n",
    "del df['Days Since Birth']\n",
    "\n",
    "#Creating new variable 'Years in Employment' and deleting column 'Days Employed'\n",
    "df['Years in Employment'] = -df['Days Employed']/365\\\n",
    "del df['Days Employed']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Question 4**\n",
    "\n",
    "- Create a **one**-dimensional NumPy array named `y` by exporting the first 5,000 observations of `Payment_Default`. (Hint: see `ravel()` function)\n",
    "- Create a NumPy array named `X` by exporting the first 5,000 observations of the following columns `Gender`, `Car`, `Real Estate`, `Children`, `Income`, `Age`, `Years in Employment`.\n",
    " \n",
    "(10 points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#Creating a one-dimensional NumPy array named y by exporting the first 5,000 observations of 'Payment_Default'\n",
    "y = np.array(df.iloc[:5000,5:6])\n",
    "y = np.ravel(y)\n",
    "\n",
    "#Creat a NumPy array named X by exporting the first 5,000 observations of all columns except 'Payment_Default'\n",
    "df1 = df.drop('Payment Default', axis=1)\n",
    "X = np.array(df1.iloc[:5000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Question 5** \n",
    "\n",
    "- Use an appropriate `scikit-learn` library we learned in class to create the following NumPy arrays: `y_train`, `y_test`, `X_train` and `X_test` by splitting the data into 70% training and 30% test datasets. \n",
    "- Set `random_state` to 0 and stratify subsamples so that train and test datasets have roughly equal proportions of the target's class labels. \n",
    "\n",
    "(5 points) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into 70% training and 30% test datasets. \n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0, stratify = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Question 6**\n",
    "\n",
    "- Create new variables by using an appropriate `scikit-learn` library we learned in class to standardize the features from the training and test datasets to mean zero and variance one. Name the new variables by appending '_scaled' to the original variable names.\n",
    "\n",
    "\n",
    "(10 points)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating new variables by standardizing the features from the training and test datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "\n",
    "sc.fit(X_train)\n",
    "\n",
    "X_train_scaled = sc.transform(X_train)\n",
    "X_test_scaled = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Problem 2 - (20 Points) {-}\n",
    "\n",
    "**Question 7**\n",
    "\n",
    "Fit the following two classifiers to the transformed training dataset using `scikit-learn` libraries.\n",
    "\n",
    "- Perceptron - name your instance `pc` set `random_state=1`\n",
    "- Logistic Regression - name your instance `lr` set `random_state=1`\n",
    "\n",
    "When initializing instances of the above classifiers only set the parameters referenced above and nothing else.\n",
    "\n",
    "(20 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(random_state=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(random_state=1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(random_state=1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Perceptron\n",
    "from sklearn.linear_model import Perceptron\n",
    "pc = Perceptron(random_state=1)\n",
    "pc.fit(X_train_scaled, y_train)\n",
    "\n",
    "#Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(random_state=1)\n",
    "lr.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 3 - (30 points) {-}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 8**\n",
    "\n",
    "- Using a method built into each of the two classifiers compute their prediction accuracies on the training data;\n",
    "- Store the accuracy values into variables named according to the following pattern: `classifier_name_accuracy_train`, e.g. you should have `lr_accuracy_train`; \n",
    "- Print the two accuracy **variables** along with their brief descriptions.\n",
    "\n",
    "(10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Perceptron = 0.496\n",
      "Accuracy of Logistic Regression = 0.558\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#Accuracy of Perceptron on the training data\n",
    "y_train_pred_pc = pc.predict(X_train_scaled)\n",
    "Perceptron_accuracy_train = accuracy_score(y_train, y_train_pred_pc)\n",
    "print(f'Accuracy of Perceptron = {Perceptron_accuracy_train:.3f}')\n",
    "\n",
    "# Accuracy of Logistic Regression  on the training data\n",
    "y_train_pred_lr = lr.predict(X_train_scaled)\n",
    "lr_accuracy_train = accuracy_score(y_train, y_train_pred_lr)\n",
    "print(f'Accuracy of Logistic Regression = {lr_accuracy_train:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Question 9** \n",
    "\n",
    "- Using a method built into each of the above classifiers compute their prediction accuracy for the test dataset\n",
    "- Store the accuracy values into variables named according to the following pattern: `classifier_name_accuracy_test`, e.g. you should have `lr_accuracy_test`. \n",
    "- Print the two accuracy **variables** along with brief descriptions.\n",
    "\n",
    "(10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Perceptron = 0.495\n",
      "Accuracy of Logistic Regression = 0.507\n"
     ]
    }
   ],
   "source": [
    "#Accuracy of Perceptron on the test data\n",
    "y_test_pred_pc = pc.predict(X_test_scaled)\n",
    "Perceptron_accuracy_test = accuracy_score(y_test, y_test_pred_pc)\n",
    "print(f'Accuracy of Perceptron = {Perceptron_accuracy_test:.3f}')\n",
    "\n",
    "# Accuracy of Logistic Regression  on the test data\n",
    "y_test_pred_lr = lr.predict(X_test_scaled)\n",
    "lr_accuracy_test = accuracy_score(y_test, y_test_pred_lr)\n",
    "print(f'Accuracy of Logistic Regression = {lr_accuracy_test:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Question 10** \n",
    "\n",
    "Using nicely formated text in Markdown comment on the accuracies computed in Questions 8 & 9 making sure you address:\n",
    "- training and test set datasets; \n",
    "- Perceptrion and Logistic Regression models. \n",
    "\n",
    "Are the results as expected, and why or why not? (Hint: You are not expected to comment on why a particular model is better.) \n",
    "\n",
    "(10 marks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**\n",
    "\n",
    "\n",
    "- training and test datasets; the datasets splited into training(%70) and test(%30) datasets so that algorithms can be trained on train dataset and tested on testing dataset.\n",
    "\n",
    "\n",
    "- Perceptrion and Logistic Regression models: Both algoriths predicted the target variable quite poorly, but logistic regression model performed relatively better as expected due to, I think, some booliean variables. \n",
    "\n",
    "- Additionally, as expected accuracy scores on testing data are not better than training data because We use the training data to train the dataset so the test dataset was totaly new dataset to the algorithm to predict.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
